<meta charset="utf-8"> 
**Logging experimental data: Part I war stories and guidance**
        John H. Williamson

[Back to the series on logging](index.md.html)        

# <a id="advice"> Advice and war stories </a>
This article condenses years of tears, frustration and lost data into some broad recommendations. In summary:

* Use a good, proven format. I recommend a sqlite database.
* Test log files thoroughly, before and during experiments.
* Write tooling to simulate, validate and backup experimental logs.
* Take care with timestamps.
* Allocate a Keeper of Logs.

## Problems
I have supervised many, many students creating experimental logs (and some professionals!) and *virtually all of them* had serious problems at some stage. There's definitely a tendency to feel that logging is just "printing out some numbers to a file" -- a few lines to be bolted onto an existing demo to make an experimental platform. Logging problems I have personally seen include:

    * Files not being flushed after writing and crashes truncating files, often to 0 bytes;
    * Older files being overwritten by new ones because of poor naming conventions;
    * Dodgy custom CSV writers that don't correctly deal with quotes, UTF8 characters, or commas in strings;
    * Files named so opaquely that linking them to the experimental trial they logically go with is error-prone and requires lots of dubious hand-editing ('log00.csv, log01.csv, log01-new.csv'...);
    * Log files that cannot easily be joined if an experiment has to be paused/restarted part-way through;
    * Simultaneous logs from devices in different time zones resulting in logs with data apparently desynchronised by hours;
    * Enormous JSON or XML files that cannot easily be opened, validated or browsed with any standard tool;    
    * Log files that are opened but never written to;
    * Critical missing data/columns from the logs;
    * Files with intermittent partial rows or other fatal formatting flaws that are difficult to even make parseable later;
    * Inappropriate data conversions:
        * Angles being converted from radians to degrees *twice* before writing (a destructive operation!)
        * Timestamps being written as floating point seconds since the epoch... in scientific notation... which end up being identical on every row;        
        * JSON strings inside CSV (!) being truncated to some arbitrary maximum length, rendering them unparseable; 
        * Floats in the range 0-1 written as integers;   
        * Missing values written as 0, infinity, 999 or multiple *different* strings.
    * Inconsistent/incomplete timestamps, including missing timezones, times recorded as frames with a variable (unknown) frame rate, times recorded since an unknown start time, times quantised to seconds, minutes or even days;    
    * Multi-threaded/multi-process writes leading to out-of-order rows or outright corruption;
    * Random seeds not being recorded and consequent irreproducible behaviour. 

Most such problems are ironed out in pilot tests; but some slip through and can ruin dozens of hours of painstaking experimental work. **This should never happen!**

## Basic lessons

* **Lesson 1**: Every experiment needs a thorough pilot *that includes a detailed postmortem of the log files*. 
    * Are the files there? Do they have enough rows? Are there consistent timestamps? Is every row parsed correctly? Do all values lie in sensible ranges? Is there more than one unique value in each column (to catch "everything is always zero" bugs)?

* **Lesson 2**: All experimental data needs accurate, full precision timestamps whenever a row of data is written.  
    * Timestamps should come from clocks synchronised with NTP to a reliable time server.
    * Ideally these should be ISO 8601 format string dates like `2022-08-30T15:46:26.585824+01:00`, with **timezone**, at least millisecond precision
        * (`date -Ins` will generate a proper timestamp at the command line in Unix systems...)
        * You can record everything in UTC (which has some advantages, particularly that lexicographic order=time order), but then it can be difficult to align lab notes or personal recollections with the data without converting to the local time first.
        * If recording in UTC, record the machine's local time zone somewhere too.
        * Recording UTC nanoseconds since the epoch as an `int64` is a *reasonable* alternative, but it is much less likely to make timing errors obvious.
    * If very precise timing is required in analysis, you may want to store full timestamps *and* a high-precision system timer, like `time.perf_counter_ns()` in Python.
    * Timestamps are a reliable way of working out what *really* happened when. 
    * Timestamps should not be: frame counts (!), milliseconds since the app started, seconds since the epoch, microdays since the birth of Mohamed...

* **Lesson 3**: Data needs to be written sequentially, in a true "ship's log" format, where later execution can never modify an earlier record. Logs are always immutable and append-only.

* **Lesson 4**: If multiple threads or processes might need to contribute to the log, a separate process/thread should be dedicated to queue and write log files. Alternatively, each process/device/thread should write its own log file and the results should be merged in a post-processing step after all experiments are concluded. Ideally, both. 

* **Lesson 5**: Experimental software needs to perform basic checks and report on the log files **every time it is used** and ideally verify and report log integrity/status **during execution**. 
    * This can be as simple as console messages like:
    ```
    Starting...
    Writing to logs/user_8_experiment_1.csv, user SONAF-ABELE
    ...
    281 lines written to logs/user_8_experiment_1_a.csv.
    281 lines read back OK.
    ```
    * This can save hours of headache (and heartache!).
    * Ideally, the logging software should do this by *reading back* the log and return stats based on what was parsed from the file, not reporting what was apparently written!

* **Lesson 6**: Pre-assign any "randomised" elements and avoid generating anything on the fly. 
    * If that isn't possible, hash known and recorded attributes (e.g. user ID+*recorded* time) to make a fixed, recoverable seed. 
    * If that isn't reasonable, at least always record the random seed/random state used on every run. Experimental work must be reproduceable.
        * Make sure all the seeds that could affect the experiment are set correctly (e.g. in Python, this might involve setting the `random` seed, the `numpy` seed *and* the `pytorch` seed!)

* **Lesson 7**: Use well-tested libraries to read and write data, even if the format seems simple. It is easy to write an invalid CSV file if you do it yourself. 

* **Lesson 8**: Estimate how much data will be logged -- even a ballpark estimate. "1000 rows/minute" or "10,000 events per trial x 18 trials". A volume estimate is essential in tracking quality of log files.
  *  Be realistic about the volume of data to be collected. If there's going to be tens of millions of rows of data, is a single JSON or CSV file really a good option? If there will be ten rows, is a Postgres database really required?

* **Lesson 9**: Write missing data explicitly. Use a consistent form for "null" that cannot be confused for real data. Don't allow nulls where there must be valid data!

* **Lesson 10**: Determine some kind of schema that can be validated. It's particularly useful to know the valid ranges of numerical data or valid category codes that might be logged. 


## Good practice
### A Keeper of Logs
If an experiment is high-value or complex, (e.g. can only be run once, requires many participants, is multi-site, uses expensive equipment, etc.) and there is more than one member of the team engaged in experimental work, it's worth designating a **Keeper of Logs**. The Keeper is responsible for making sure the log is in good order. This includes:

* Checking the log format with the team well in advance of the experimental work;
* Performing test runs with the logging software prior to experiments and verifying logging produces sensible files that are parsed and processed correctly;
* Monitoring and verifying logging is operating normally during experiments (if possible);
* Verifying the intactness and completeness of logs at the end of each day -- at minimum, checking that the log files have sensible numbers of rows added to them, but ideally more thorough spot-checks;
* Backing up logs on a daily basis;
* Recording experimental notes into the log;
* Storing and distributing the log files following the experiment.

### Simulation and mock logs
It is useful to have some automated way of creating "mock logs" quickly, even if this just creates the right files and writes some random data in. This should be a part of the development of the experimental software.

In high-value experiments, writing some form of more sophisticated simulator is usually an investment that pays off. It's important that the simulator *writes logs using the actual logging code*, in the same format. Pre-testing analysis and validation scripts on the simulated logs can remove a great deal of risk and accelerate the analysis phase significantly. It can also be useful to deciding on sample sizes or other experimental variables -- depending on how authentic the simulation is. Obviously, a realistic simulator may be time-consuming to engineer, but even a vaguely plausible one can be useful. Ideally, a pre-trial simulator would run through (all) the experimental states that would occur during every trial and generate similar quantities of data with similar distributions to the planned trials.

### Validator scripts
It's worth taking the time to write a simple validator script, either as part of the experimental software or as an external script that's run frequently. This should read the logs and validate/report back basic information, such as:

* Verifying the log parses correctly;
* Reporting total number of rows written;
* Listing user names or experimental runs (whatever logical unit makes sense for the given experiment);
    * Reporting the number of log entries per user/trial/etc. to make sure these are within reason;
* Verification that all values lie inside ranges given by a schema;
* Checking for null or repeated values/rows;

This isn't a substitute for a keen eye spot-checking the data, but it's a good safety precaution. It's particularly useful to have a log format that can easily be queried. A database is ideal in this regard, but tools like `jq` can provide similar functionality for JSON files. A format that is hard to query effectively is riskier to use.

### Network and multi-device logging
Some experiments involve distributed logging with multiple devices operating simultaneously. This adds an order-of-magnitude of complexity.

* Use NTP time synchronisation if at all possible. 
    * If not, make sure all clocks involved are carefully synchronised. On mobile devices, a GPS clock is the most reliable way of doing this if it is available.
* Make sure every device that might be involved in log records has a reliable and consistent UUID.
    * Most experiments will have devices (e.g. a computer) that have both hardware UUIDs and logical names ("unit5"). Record them both in logs, as well as any other identifying information.
* It is usually easier and safer to log locally and merge post-hoc than to log centrally. In simple cases, logs can be simply be merged and resorted by timestamp.
* If multiple devices or processes *are* being logged to a central server or process, **log both locally and on the server**. Being able to reconstruct logs (in the worst case) from individual local logs is invaluable, especially if synchronisation goes awry.
* Multiple processes should always transmit heartbeat messages (e.g. once a second), which should be monitored and logged by the central process -- usually a heartbeat message would just contain a `(UUID, sequence_number, timestamp)` tuple or something similar.
* Record exact version numbers of interpreters/libraries/etc. somewhere in the log on each execution of the software.
* ZeroMQ or similar message queue brokers are useful for managing communication between devices and a logging process.

### Backups

* Logs should be backed up after every trial, or daily, whichever is more reasonable.
* Checking logs into version control (e.g. `git`) on a remote server is one fairly robust way of doing this.
    * This can break down for very large log files.
* If not using version control, unless storage space is at a premium, it's advisable to archive the entire current log set in a dated archive (`gft-expt-daily-2022-08-30.zip`) and keeping every daily snapshot on a remote server.
    * This avoids the nightmare scenario when a critical software failure overwrites/corrupts the logs and the backup archive is overwritten before anyone notices!

# File formats

There are a few formats commonly used for logging data:

* CSV: Easy to open in Excel, Pandas or R to analyse. Plain text and easy to read and manipulate, but limited in structure and requires file management. 
* JSON: Flexible, easy to serialise to; but big files, easy to be sloppy and rely on direct serialisation, still requires file management. Annoying to read for humans.
* YAML: Like JSON, but even easier to shoot yourself in the foot. Easier to read and check by hand, however. 
* XML: Like JSON, but even worse in almost every way -- bulky, hard to read/check by hand, annoying to work with.
* .txt: Your own custom text file: the worst text file of all and the most error prone. Chances are there will be formatting issues and edge cases galore. Do not do this.

None of these are ideal. The key issues with "collection of text files" logging:

* Files still need to be managed. What is a file unit? One user? One task? What if multiple kinds of data need logged? Will they be squeezed into one file, or will the analysis need to stitch things together? How are files named? 
    * A directory filled with `0001.csv, 0002.csv, 0003.csv, 0003_partial_a.csv, 0004.csv` is a nightmare to deal with.    
    * It's even worse if you have to explain it someone else, like yourself in a year's time.
* Formats like CSV can't easily record everything that might be useful to capture and are limited to a "single table" model -- this often requires either multiple interlinked files or lots of repeated data.
* Conversely, formats like JSON/XML/YAML are so flexible that it easy to store up an analysis nightmare. You may well end up storing everything and the kitchen sink, and have no easy way of extracting what you need.
    * Loggers which are essentially just "serialise the program state every n seconds" are not usually a good design choice.
    * There is no good lazy solution!
* Custom formats require parsers that exactly match the generated/written data -- and problems here are usually discovered too late. 
* Non-line-based textual formats generally require you to parse the entire file before doing even the simplest analysis (JSON/XML is worst for this). This is a problem if you have 10GB of data!
    * The alternative -- thousands of tiny JSON files -- isn't much more attractive.
* No schemas (unless you use things like [JSON schema](https://json-schema.org/), [StrictYAML](https://github.com/crdoconnor/strictyaml) or an XML schema).
    * Great for rapid prototyping!
    * Terrible for everything else!        

## Conclusion
* If you're going to write text files and your experiment is simple, CSV (using a solid, proven library) is a decent choice. 
* Dividing data into files requires thought and care.
* Logging isn't something you can slap together a few hours before the study starts -- it requires preparation.
* All text file solutions have problems.
* Consider using a database instead -- see the next post in this series:

[Part II: logging to sqlite](part_ii_logging_to_sqlite.md.html)


[John H Williamson](https://johnhw.github.io)

[GitHub](https://github.com/johnhw) / [@jhnhw](https://twitter.com/jhnhw)

<link rel="stylesheet" href="../global/style.css" type="text/css" ></link>
<script>
window.markdeepOptions = {tocStyle:'short'};</script>
<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>