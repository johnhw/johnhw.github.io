<meta charset="utf-8"> 
**Logging experimental data**
        John H. Williamson

![-](imgs/header.png)

# Experimental logging
I often deal with experimental trials in human-computer interaction: an experiment is set up where a user interacts with some software, performing a sequence of tasks, and the interactions are logged for later analysis. For example:

* clicking on a sequence of targets of different sizes and locations;
* reading a document with types of scrolling;
* driving a (simulated) car with different kinds of controllers;

and so on. 

There are lots of other experimental instruments that are used in these studies (questionnaires, video recording, interviews, etc.) but usually we also want to capture what happens inside the experimental software by writing out log files. These usually look something like this:

```
    log_user_1.csv
        Time,action,x,state
        20.4321,OK clicked,245,102
        10.330,Back pressed,100,19

    log_user_2.csv
        Time,action,x,state
        ...
```

This ought to be trivial. It doesn't seem to be.



# <a id="advice"> Advice and war stories </a>

## Problems
I have supervised many many students doing this type of logging task (and some professionals!) and *virtually all of them* had serious problems at some stage. Ones I have seen:

    
    * Files not being flushed after writing and crashes truncating files, often to 0 bytes;
    * Older files being overwritten by new ones because of poor naming conventions;
    * Dodgy custom CSV writers that don't correctly deal with quotes, UTF8 characters, or commas in strings;
    * Files named so opaquely that linking them to the experimental trial they logically go with is error-prone and requires lots of dubious hand-editing ('log00.csv, log01.csv, log01-new.csv'...);
    * Log files that cannot easily be joined if an experiment has to be paused/restarted part-way through;
    * Simultaneous logs from devices in different time zones resulting in logs with data apparently desynchronised by hours;
    * Enormous JSON or XML files that cannot easily be opened, validated or browsed with any standard tool;    
    * Log files that are opened but never written to;
    * Critical missing data/columns from the logs;
    * Files with intermittent partial rows or other fatal formatting flaws that are difficult to even make parseable later;
    * Inappropriate data conversions:
        * Angles being converted from radians to degrees *twice* before writing (a destructive operation!)
        * Timestamps being written as floating point seconds since the epoch... in scientific notation... which end up being identical on every row;        
        * JSON strings inside CSV (!) being truncated to some arbitrary maximum length, rendering them unparseable; 
        * Floats in the range 0-1 written as integers;   
        * Missing values written as 0, infinity, 999 or multiple *different* strings.
    * Inconsistent/incomplete timestamps, including missing timezones, times recorded as frames with a variable (unknown) frame rate, times recorded since an unknown start time, times quantised to seconds, minutes or even days;    
    * Multi-threaded/multi-process writes leading to out-of-order rows or outright corruption;
    * Random seeds not being recorded and consequent irreproducible behaviour. 

Most such problems are ironed out in pilot tests; but some slip through and can ruin dozens of hours of painstaking experimental work. **This should never happen!**

## Basic lessons

* Lesson 1: Every experiment needs a thorough pilot *that includes a detailed postmortem of the log files*. 
    * Are the files there? Do they have enough rows? Are there consistent timestamps? Is every row parsed correctly? Do all values lie in sensible ranges? Is there more than one unique value in each column (to catch "everything is always zero" bugs)?

* Lesson 2: All experimental data needs accurate, full precision timestamps whenever a row of data is written.  
    * Timestamps should come from clocks synchronised with NTP to a reliable time server.
    * Ideally these should be ISO 8601 format string dates like `2022-08-30T15:46:26.585824+01:00`, with **timezone**, at least millisecond precision
        * (`date -Ins` will generate a proper timestamp at the command line in Unix systems...)
        * You can record everything in UTC (which has some advantages, particularly that lexicographic order=time order), but then it can be difficult to align lab notes or recollections with the data without converting to the local time first.
        * If recording in UTC, record the machine's local time zone somewhere too.
    * If very precise timing is required in analysis, you may want to store full timestamps *and* a high-precision system timer, like `time.perf_counter_ns()` in Python.
    * Timestamps are a reliable way of working out what *really* happened when. 
    * Timestamps should not be: frame counts (!), milliseconds since the app started, seconds since the epoch, microdays since the birth of Mohamed...

* Lesson 3: Data needs to be written sequentially, in a true "ship's log" format, where later execution can never modify an earlier record. Logs are always immutable.

* Lesson 4: If multiple threads or processes might need to contribute to the log, a separate process/thread should be dedicated to queue and write log files. Alternatively, each process/device/thread should write its own log file and the results should be merged in a post-processing step after all experiments are concluded. Ideally, both. 

* Lesson 5: Experimental software needs to perform basic checks and report on the log files **every time it is used** and ideally verify and report log integrity/status **during execution**. 
    * This can be as simple as console messages like:
    ```
    Starting...
    Writing to logs/user_8_experiment_1.csv, user SONAF-ABELE
    ...
    281 lines written to logs/user_8_experiment_1_a.csv.
    281 lines read back OK.
    ```
    * This can save hours of headache (and heartache!).
    * Ideally, the logging software should do this by *reading back* the log and return stats based on what was parsed from the file, not reporting what was apparently written!

* Lesson 6: Pre-assign any "randomised" elements and avoid generating anything on the fly. 
    * If that isn't possible, hash known and recorded attributes (e.g. user ID+*recorded* time) to make a fixed, recoverable seed. 
    * If that isn't reasonable, at least always record the random seed/random state used on every run. Experimental work must be reproduceable.
        * Make sure all the seeds that could affect the experiment are set correctly (e.g. in Python, this might involve setting the `random` seed, the `numpy` seed *and* the `pytorch` seed!)

* Lesson 7: Use well-tested libraries to read and write data, even if the format seems simple. It is easy to write an invalid CSV file if you do it yourself. 

* Lesson 8: Estimate how much data will be logged -- even a ballpark estimate. "1000 rows/minute" or "10,000 events per trial x 18 trials". A volume estimate is essential in tracking quality of log files.
  *  Be realistic about the volume of data to be collected. If there's going to be tens of millions of rows of data, is a single JSON or CSV file really a good option? If there will be ten rows, is a Postgres database really required?

* Lesson 9: Write missing data explicitly. Use a consistent form for "null" that cannot be confused for real data. Don't allow nulls where there must be valid data!

* Lesson 10: Determine some kind of schema that can be validated. It's particularly useful to know the valid ranges of numerical data or valid category codes that might be logged. 


## Good practice
### A Keeper of Logs
If an experiment is high-value or complex, (e.g. can only be run once, requires many participants, is multi-site, uses expensive equipment, etc.) and there is more than one member of the team engaged in experimental work, it's worth designating a **Keeper of Logs**. The Keeper is responsible for making sure the log is in good order. This includes:

* Checking the log format with the team well in advance of the experimental work;
* Performing test runs with the logging software prior to experiments and verifying logging produces sensible files that are parsed and processed correctly;
* Monitoring and verifying logging is operating normally during experiments (if possible);
* Verifying the intactness and completeness of logs at the end of each day -- at minimum, checking that the log files have sensible numbers of rows added to them, but ideally more thorough spot-checks;
* Backing up logs on a daily basis;
* Recording experimental notes into the log;
* Storing and distributing the log files following the experiment.

### Simulation and mock logs
It is useful to have some automated way of creating "mock logs" quickly, even if this just creates the right files and writes some random data in. This should be a part of the development of the experimental software.

In high-value experiments, writing some form of more sophisticated simulator is usually an investment that pays off. It's important that the simulator *writes logs using the actual logging code*, in the same format. Pre-testing analysis and validation scripts on the simulated logs can remove a great deal of risk and accelerate the analysis phase significantly. It can also be useful to deciding on sample sizes or other experimental variables -- depending on how authentic the simulation is. Obviously, a realistic simulator may be time-consuming to engineer, but even a vaguely plausible one can be useful. Ideally, a pre-trial simulator would run through (all) the experimental states that would occur during every trial and generate similar quantities of data with similar distributions to the planned trials.

### Validator scripts
It's worth taking the time to write a simple validator script, either as part of the experimental software or as an external script that's run frequently. This should read the logs and validate/report back basic information, such as:

* Verifying the log parses correctly;
* Reporting total number of rows written;
* Listing user names or experimental runs (whatever logical unit makes sense for the given experiment);
    * Reporting the number of log entries per user/trial/etc. to make sure these are within reason;
* Verification that all values lie inside ranges given by a schema;
* Checking for null or repeated values/rows;

This isn't a substitute for a keen eye spot-checking the data, but it's a good safety precaution.

### Network and multi-device logging
Some experiments involve multiple devices operating simultaneously. This adds complexity.

* Use NTP time synchronisation if at all possible. 
    * If not, make sure all clocks involved are carefully synchronised. On mobile devices, a GPS clock is the most reliable way of doing this if it is available.
* Make sure every device that might be involved in log records has a reliable and consistent UUID.
    * Most experiments will have devices (e.g. a computer) that have both hardware UUIDs and logical names ("unit5"). Record them both in logs, as well as any other identifying information.
* If multiple devices or processes are being logged to a central server or process, **log both locally and on the server**. Being able to reconstruct logs (in the worst case) from individual local logs is invaluable, especially if synchronisation goes awry.
* Multiple processes should always transmit heartbeat messages (e.g. once a second), which should be monitored and logged by the central process -- usually a heartbeat message would just contain a `(UUID, sequence_number, timestamp)` tuple or something similar.
* Record exact version numbers of interpreters/libraries/etc. somewhere in the log on each execution of the software.
* ZeroMQ or similar message queue brokers are useful for managing communication between devices and a logging process.

### Backups

* Logs should be backed up after every trial, or daily, whichever is more reasonable.
* Checking logs into version control (e.g. `git`) on a remote server is one fairly robust way of doing this.
    * This can break down for very large log files.
* If not using version control, unless storage space is at a premium, it's advisable to archive the entire current log set in a dated archive (`gft-expt-daily-2022-08-30.zip`) and keeping every daily snapshot on a remote server.
    * This avoids the nightmare scenario when a critical software failure overwrites/corrupts the logs and the backup archive is overwritten before anyone notices!

# File formats

There are a few formats commonly used for logging data:

* CSV: Easy to open in Excel, Pandas or R to analyse. Plain text and easy to read and manipulate, but limited in structure and requires file management. 
* JSON: Flexible, easy to serialise to; but big files, easy to be sloppy and rely on direct serialisation, still requires file management. Annoying to read for humans.
* YAML: Like JSON, but even easier to shoot yourself in the foot. Easier to read and check by hand, however. 
* XML: Like JSON, but even worse in almost every way -- bulky, hard to read/check by hand, annoying to work with.
* .txt: Your own custom text file: the worst text file of all and the most error prone. Chances are there will be formatting issues and edge cases galore. Do not do this.

None of these are ideal. The key issues with "collection of text files" logging:

* Files still need to be managed. What is a file unit? One user? One task? What if multiple kinds of data need logged? Will they be squeezed into one file, or will the analysis need to stitch things together? How are files named? 
    * A directory filled with `0001.csv, 0002.csv, 0003.csv, 0003_partial_a.csv, 0004.csv` is a nightmare to deal with.    
    * It's even worse if you have to explain it someone else, like yourself in a year's time.
* Formats like CSV can't easily record everything that might be useful to capture and are limited to a "single table" model -- this often requires either multiple files or lots of repeated data.
* Formats like JSON/XML/YAML are so flexible that it easy to store up an analysis nightmare. You may well end up storing everything and the kitchen sink, and have no easy way of extracting what you need.
    * Loggers which are essentially just "serialise the program state every n seconds" are not usually a good design choice.
    * There is no good lazy solution!
* Custom formats require parsers that exactly match the generated/written data -- and problems here are usually discovered too late. 
* Non-line-based textual formats generally require you to parse the entire file before doing even the simplest analysis (JSON/XML is worst for this). This is a problem if you have 10GB of data!
    * The alternative -- thousands of tiny JSON files -- isn't much more attractive.
* No schemas (unless you use things like [JSON schema](https://json-schema.org/), [StrictYAML](https://github.com/crdoconnor/strictyaml) or an XML schema).
    * Great for rapid prototyping!
    * Terrible for everything else!        

## Conclusion
* If you're going to write text files, CSV (using a solid library) is a decent choice. 
* Dividing data into files requires thought and care.
* Logging isn't something you can slap together a few hours before the study starts -- it requires preparation.
* All text file solutions have problems.

# Databases and logging to sqlite
My view is that experimental logging should be **recorded to a database** as it is being logged. Databases provide guaranteed clean transactions, make it easy to manage multiple related tables, and are extremely fast to query when performing analyses. The convenience of plain text manipulations like `tail logs.csv` is easily replicated with modern tools.

## Database choice
In most cases, I'd use `sqlite` as the database. It's fast, lightweight, available for pretty much everywhere, and there is excellent tooling to explore and manipulate databases. The only case where a heavier duty database like `postgres` might be justifiable would be in concurrent trials (e.g. many simultaneous users in crowdsourced trial), as `sqlite` is designed for single process writers.

I've encountered some logging projects using NoSQL like MongoDB. These are sometimes easier to setup (less schema design etc.) but can be a poor fit to the problem of logging. In particular, a schema is good way of designing a usable and well-thought out log.

## sqlite
With `sqlite` you have:
* a single log file to deal with;
* trivial scaling to many gigabytes of experimental data;
* relatively compact storage;
* reliable transactions, full SQL queries, indices, multiple tables;
* automatic constraints and integrity guarantees (e.g. unique usernames, ages in range 18-100) if you want to use them;
* quick and easy ways of previewing data;
* support for [spatial queries](https://www.sqlite.org/rtree.html), [text search](https://www.sqlite.org/fts5.html) and [JSON](https://www.sqlite.org/json1.html)

`sqlite` is:
* extremely efficient;
* a standard, free and well-documented format that many tools can open and work with;
* available in virtually all language and OS combinations ([C, C++](https://www.sqlite.org/index.html), [Python](https://docs.python.org/3/library/sqlite3.html) -- built in, [JavaScript](https://github.com/sql-js/sql.js), [Java](https://www.sqlitetutorial.net/sqlite-java/), [R](https://solutions.rstudio.com/db/databases/sqlite/), [MATLAB](https://www.mathworks.com/help/database/ug/sqlite.html)).
    * [You don't even need to have an operating system to use sqlite](https://www.sqlite.org/vfs.html).

### sqlite-utils and datasette

In the following discussion I'm going to assume of the use of two excellent utilities by Simon Willison: 
* [`sqlite-utils`](https://github.com/simonw/sqlite-utils), which provides easy importing/exporting to CSV, schema modifications and database information from the command line;
* [`datasette`](https://datasette.io/), an outstanding utility that instantly pops up a webserver serving the contents of a sqlite database as a web page, where you can browse and query data instantly. You can even run this in *entirely in browser* with [datasette-lite](https://lite.datasette.io/) which makes sharing and disseminating datasets easy. **I can't recommend datasette highly enough.**

But even with plain old `sqlite3` installed, you can run simple SQL scripts to check data, export CSV, and print statistics.

## sqlite logs
### FAQ

* Won't I have to choose a schema then? And then manually create tables and all that jazz?
    * Yes. Creating a schema for the data is key step in designing a log format; **you can't skip it!** 
    * Most logging schemas are very simple and follow a basic pattern; there are examples below.
    * *If you can't write down a schema, you aren't ready to start logging data.*
    * Creating the initial empty tables should be part of the experimental software.
    * In the worst case, just writing rows to `sqlite` table(s) with a timestamp and a JSON blob is usually better than dumping to a file.

* [Isn't a plain text format best](https://plain-text.co/)?
    * In many ways, it is. But text formats are most useful for analysis, not as the primary record. Generate plain text *from* a database. This is a scalable and robust way to deal with logs.
    * Tools like `datasette` make sqlite databases so easy to explore that it mitigates many of the reasons to stick to a plain text format.

* But what if I need to record things that aren't just dates, numbers or strings?
    * Ideally, you'd avoid doing this. The more carefully the schema is planned, and the simpler it is, the less pain there will be when analysing.
    * Sometimes this isn't possible; maybe you need to store a tree of options, or a numpy array, or a collection of configuration switches.
    * In that case, just store a JSON string in the database. `sqlite` [can even parse the JSON and query it later](https://www.sqlite.org/json1.html), if you wish.
    * Do think carefully before blindly dumping JSON: Do I need this? Will I be able to use this later?

* Do I really need foreign keys, constraints, views, joins, triggers?
    * No. You almost certainly don't. Most database logs are plain tables. A logger doesn't need anything fancy.
    * Constraints (like `UNIQUE`, `NOT NULL` or `CHECK(width>0)`) are sometimes useful to ensure integrity.
    * It can be handy to create views *after* an experiment is complete to make it easier to explore the data.

* I need to store lots of large binary files in my experiments, like images.
    * You *can* store these as BLOBs inside a `sqlite` database.
    * But this can lead to excessively large database files and degraded performance.
    * Instead, a better option is to write them as separate files, but index the filenames in the database (with the rows they correspond to, so they have timestamps and so on).

* My funder/colleague/archival service requires a CSV file!

  * No problem. Just generate CSV files (or whatever it is you need) *programatically* from the log database. This is easy with `sqlite-utils`. 
  * By writing a script to generate the transformed files, you can reliably reproduce all of the data from the database. 
  * It's fine to do the analysis on the CSV files, or a subset of them, if that's what works best -- but it's not great as a format for the master data. 



* How will I analyse a `sqlite` file?
    * You can perform exploratory queries with tools like `datasette` or the `sqlite3` command line tool.
        * This is useful for checking things like average task times, or completion rates per group, etc.
    * If you use Python, you can *directly* open a `sqlite` file in `Pandas` with `read_sql_query` and get a DataFrame back from a query (e.g. `read_sql_query('SELECT * FROM LOG')`)
        * Other analysis tools like R offer similar capabilities.
    * Alternatively, use `sqlite-utils` to export tables (or queries) to CSV and work with those.
        * The key thing is that any generated CSV files be created *programmatically* from the master database -- write scripts that generate the "second stage" analysis data from the raw logs.

* How do I deal with version control of the logs?
    * A binary database is not well suited to version control (at least not `git`), but version control is potentially useful when dealing with log files, at the very least as a reliable and easily distributed form of backup.
    * A simple workaround is to `.gitignore` the binary database, and dump the sqlite file to a plain text file and version control *that*, leaving the real sqlite file out of the VCS.

```
    sqlite3 my_logs.db .dump > my_logs.sql
    git add my_logs.sql
    git commit -m "Updated database after experimental run for user KINED-ABABO"
```

    * If ever required, the `.sql` file can be reconstituted into a replica of the binary database:

```
    sqlite3 my_logs.db < my_logs.sql
```

* What if I need to change the log format/schema partway through?
    * You shouldn't! Pilots should have been conducted to verify the log format is good.
    * But we all know this happens sometimes.
    * Procedure:
        1. Track the version number of your log formats carefully (`log-format-v1`, `log-format-v2`, etc.) (and write it in a README file!)
        1. Check the existing log database into version control and/or backup the logs.
        1. Modify the logging software to log in the new format.
        1. Write a script that uses `sqlite-utils` or `python` to modify the schema, like `log-format-v1-to-v2-db.sh` (see the example below).    
        1. Check both the new version of the logging software and the conversion script into version control.
        1. Run the script and convert the database to `log-format-v2`.
        1. Check the new `log-format-v2` database into version control (or backup).
        1. Continue logging.

* Example:
```
[log-format-v1-to-v2.sh]
# modifies the "log" table
# adds target_origin and target_final; removes target; renames name to username
sqlite-utils add-column my_logs.db log target_origin text
sqlite-utils add-column my_logs.db log target_final text
sqlite-utils transform my_logs.db log --drop target
sqlite-utils transform my_logs.db log --rename name username

---

# dump the current data and commit it
sqlite-utils dump my_logs.db > mylogs.sql
git add log-format-v1-to-v2-db.sh
git add my_logs.sql
git commit -m "Log format version v1 -> v2"

# <--- modify logger.py to use the new format
git add logger.py
# perform the conversion
./log-format-v1-to-v2.sh

# dump the new version and commit it
sqlite-utils dump my_logs.db > mylogs.sql
git add my_logs.sql
git commit -m "Log format now in version 2"

# carry on...
```

# Prototypes for log table structure

## What does a typical log database look like?

Every experiment will have its own needs. But there are several common table structures I've used:

> In these examples I've replaced timestamps with `x` to make the tables readable.
> In practice they'd be ISO8601 timestamps, as described in Section 1.

The basic tables are usually `users, trials, states, log, notes`.

The simplest experiments might just have a single `log` table, and maybe a `notes` table to record additional information. More complicated experiments need a bit more structure.

### users
Most experimental trials that I work with involve users. Users are generally "enrolled" in a trial and then are assigned to undertake some or all of the experimental tasks. Typically, they have some associated metadata such as demographics or anthropometrics.

`users` gives each user a unique ID, and might record demographic or other per-user data (e.g. assignment to a treatment group, or a counterbalancing task order). It's useful to have a column to record if a user is a "test" or a from a real trial (or simulated/test/pilot/real), so
that you can safely test the final database without polluting the results.

```
    Timestamp   UserID  AgeGroup    IdGender    PrefHand    Group   eType    
    ----------------------------------------------------------------------
    x           S01     18-25       X           R           A       sim
    x           T11     18-25       X           R           A       test  
    x           IKL     25-34       M           R           B       pilot
    x           ABG     18-25       M           L           A       real 
    x           QRF     18-25       X           R           A       real  
    x           PCQ     25-35       F           R           B       real
```


### trials
A simple table that records logical "trials" (i.e. one whole execution of an experiment, whatever that means) and the user taking them. A `user_trials` table can be created if there are multi-user trials involved.

```
    Timestamp   TrialID     User    eType    Notes
    --------------------------------------------------------------------------
    x           0           S01     sim      Simulation run 1
    x           1           T11     test     checking if shutdown bug fixed...
    x           2           ABG     pilot    Pilot 1
    x           3           ABG     pilot    Pilot 2
    x           4           T21     test     Testing node restart bug fix 
    x           5           QRF     real     First experiment
    x           6           TPQ     real     Second experiment
```



### log
`log` -- the primary log of activity. This might well be split into multiple tables in a more complex experiment, but a simple experiment might be served with a single table. Note the `ExptState` column, which tracks
where we are within the experiment structure.

```
    Timestamp   Action  User    PixelSize   PixelDist   TargetSeq    ExptState  TrialID  
    ------------------------------------------------------------------------------------
    x           HIT     ABG     10          100         18           A/9/3/1      2
    x           MISS    ABG     15          100         19           A/9/3/1      2
```


### experiment_state
`experiment_state` records the changes in the experimental state (for example, one task being completed). An experiment generally operates as a big state machine. Here, nested states are represented with a path-like format:

```
    Timestamp   PrevState       NewState            Action          TrialID
    -----------------------------------------------------------------------
    x           START           Pre/Enrolled        UserEnrolled      7
    x           Pre/Enrolled    Pre/A/GroupAInfo    ClickOK           7
    x           Pre/A/GroupAInfoA/1/1/1             ClickOK           7
    ...    
    x           A/9/3/1         A/9/3/2             TimedOut          7
    x           A/9/3/2         PAUSED              PausePressed      7
    x           PAUSED          A/9/3/2             PausePressed      7
    x           A/9/3/2         A/9/3/3             Complete          7
    
    x           A/9/3/3         A/10/Info           TimedOut          7
    ...
    x           A/20/3/3        Exit/Debrief        ExptComplete      7
    x           Exit/Debrief    Exit/Exit           ClickOK           7
    x           Exit/Exit       COMPLETE            ClickOK           7
```

Special states might include "START, COMPLETE, ABORT, PAUSED".

### notes
`notes` is just free-form notes with timestamps (like a traditional lab book), useful for keeping notes on experimenter observations, technical problems, notable behaviour, etc.

```
    Timestamp   Experimenter    Note
    ------------------------------------------------------------------------------
    x           John            Sensor B was flaky for first 30 seconds.
    x           John            Participant had to take break around 15 mins in.
    x           John            User ABQ swapped to 'flymo' for this trial when 'zoomer' crashed.
    x           John            Crash. Resumed from snapshot 5... 
```

## Other useful tables

There are a few other tables that are often useful to have:

### runs
`runs` records the starting and stopping of the experimental software itself and the state of the "apparatus". It's useful to record in this table:
* The UUID/name/IP of the machine, if multiple machines/servers might be used (e.g. in case there is an issue with one machine detected later)
* The version of interpreter/libraries/etc. (if relevant)
* An identifier for the exact version of the experimental software running (e.g. the Git SHA). 
* The experimenter ID (if there's more than one person running experiments)
* Hardware serial numbers etc. if external sensors or other equipment are used.
* Random seeds, if they are used (see above)

```
    Timestamp   Hostname    Version    GitSHA       Action          Seed    Exper.  TrialID
    ---------------------------------------------------------------------------------------
    x           jupiter     3.9.12       0890dead     START        39031951 John    7
    x           jupiter     3.9.12       0890dead     SHUTDOWN     91831202 John    7
    x           jupiter     3.9.12       0890dead     START        89794541 Jane    8
    x           jupiter     3.9.12       0890dead     SHUTDOWN     66635163 Jane    8
```

### saves
A useful extension is to record "resume state" (a snapshot of the complete state of the experimental software serialised -- the "save game" state) in a `saves` table, so that trials can be easily restarted in this case of a software problem, or a user issue (like a bathroom break). This is usually written as a JSON string, perhaps at intervals of two minutes or so, or after a completion of a task state. Obviously, the experimental software needs to be written carefully such that it can safely resume from such a snapshot!

```
    Timestamp   TrialID User    ResumeState
    -------------------------------------------
    x           7       ABG     {...
    x           7       ABG     {...
    x           7       ABG     {...
    x           7       ABG     {...
    x           8       QRT     {...    
    x           8       QRT     {...    
    x           8       QRT     {...    
```

In this example, the user and trial are also indexed in the table, making it easy to resume a specific user if interrupted (e.g. `python experiment.py resume QRT` might restart user QRT's trial from the last place it had recorded). This table can also be useful if you need to reconstitute the software as it appeared to the user at a given moment later in the analysis phase -- if you have relatively frequent snapshots recorded in this table, you can see what exactly had happened.

It's not necessary to store this in the same file as the log database (maybe it's better practice to separate it into another sqlite DB, especially if the snapshots are large), but some form of reliable, timestamped, persistent store should be used.



### media

`media` contains references to external media, like videos, audio recordings, screenshots, big tensors, etc. that might be recorded alongside but outside of the database. In some cases this might be better merged into the main log, but in others it makes more sense as one or more separate tables.

```
    Timestamp  MediaFile            SyncTime    ExptState   TrialID
    ---------------------------------------------------------------
        x       video/qrt-4-1.mp4     0.0       START         7
        x       imgs/qrt-1.png       null       A/3/9/1       7
        x       imgs/qrt-2.png       null       A/4/3/3       7        
```


### debug_log
`debug_log` includes the output of the ordinary debug logs that would normally be written to `stderr` (including exception tracebacks, etc.). 

```
    Timestamp   Level       LogMsg
    ----------------------------------------------------------
    x           Debug       Starting up 
    ...
    x           Warning     Module xxx...
```

This can come in very useful if there do turn out to be problems with the logger.

## Ideas and examples

There are several useful ideas in these tables:

* Give users randomised, distinct and ideally pronounceable pseudonyms (like ABQ or QUROT-ABEBE), **not sequential numbers or easily confused codes**. 
* Record accurate, complete timestamps everywhere. This can be invaluable in piecing together what really happened if you forgot to record something.
* Refer to logical experimental states with a *path-like notation*. For example "A/9/3/1" might mean "group A, trial 9, condition 3, repetition 1"; "A/10/Info" might mean "group A, trial 10, information page". See below for a more thorough discussion of path notations for states.
* Record changes of this logical state path in a separate table, and store the current state along with the primary log(s).
* Think about how you will stop and resume experiments: you should have some form of snapshot state that can be serialised and resumed from. A good test of this is to always start a new trial by writing a snapshot to the log, and immediately resuming from it.

### Example queries
Some example queries using the table structure above:

* The number of "real" (not test/pilot/simulated) users `SELECT COUNT(*) FROM USERS WHERE etype=real` 
* The average task time for task A/3/2 `SELECT MEAN(TaskTime) FROM LOG WHERE ExptState="A/3/2"` 
* The max task time for any of the tasks in group A `SELECT MAX(TaskTime) FROM LOG WHERE ExptState LIKE "A/"` 
* The number of log entries for each user. `SELECT COUNT(*) FROM log GROUPBY user` 
* The number of times each  machine was started `SELECT COUNT(*) FROM runs WHERE action="START" GROUPBY hostname`


# Tips and tools for working with sqlite logs

## sqlite setup
* You don't really need to configure `sqlite` beyond creating the SQL schema.
* However, it is useful to enable write-ahead logging (WAL) to improve database performance with `PRAGMA journal_mode=WAL;`
* Setting `PRAGMA synchronous=NORMAL` when using WAL will increase speed dramatically for frequent writes, at the cost of a possible small data loss if the whole system (not just your code) crashes (loses power, blue screens etc.). Generally, this is the option to use for logging.
* If you are writing log rows *very* frequently, then autocommits (the sqlite default) may cause performance issues. You can mitigate this by batching up a bunch of rows into a single transaction, with `BEGIN` and `COMMIT` before and after the transaction.    

## Creating the tables
* A single `create_tables.sql` file that constructs the database is a good way to specify the schema.
    * This keeps it separate from the code that does the logging.
    * This can either be run from the logging software, or simply from the command line or shell script.
    * `sqlite3 logs.db < create_tables.sql` will create a new log instance.

## Version 
* Having a "singleton" table (a table with just one row) can be useful to record the version of a log file format.
    * Many experimental platforms go through several revisions, and this can be a source of chaos at analysis time. 
* Creating this as part of `create_tables.sql` is a good way of recording this information:

```sql
...
CREATE TABLE version (creation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP, 
                    version_number TEXT, comment TEXT);
INSERT INTO version VALUES "v8", "Version with support for multiple users";
...
```

* You can even store a README here:

```sql
CREATE TABLE version (creation_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP, 
                      version_number TEXT, comment TEXT, readme TEXT);
INSERT INTO version VALUES "v8", "Version with support for multiple users",  "
# Log file format
* Author: JHW, University of Glasgow, August 2023 my@email.address

This log file records experimental results from an experiment 
measuring reaction times using different pointing devices.

## Tables
* `users` records...

";
```

## Schemas
* SQL schemas are powerful, and you can specify useful integrity checks when the tables are created. 
* Apart from their use in ensuring the logging software does not go awry, they can be compact documentation for what data should be recorded in each column. 
* A self-documenting `create_tables.sql` file is good practice.
* Handy integrity checks:
    * `NOT NULL` is useful to ensure a column is always entered
    * `UNIQUE` is useful for identifiers that should always be distinct, like user IDs in a `users` table
    * `IN` integrity checks like `CHECK (col IN ('A', 'B', 'C'))` are helpful for categorical data
    * `CHECK(age>0 AND age<200)` range checks are useful for numerical data
    * `DEFAULT=1` default values can be useful for columns that rarely differ
* **You can write comments** in a sqlite schema using `--` as a comment separator. These comments are stored in the database, and can be used as a way of documenting the logs.

For example:

```sql
    CREATE TABLE users (
        -- this table records each user in the trial
        time_stamp DATE, -- timestamp user was enrolled in the trial
        user_id TEXT NOT NULL UNIQUE CHECK(LEN(user_id)=3),  -- three letter user id, like ABQ
        years_experience INT CHECK(years_experience>=0),     -- years of experience using a mouse
        id_gender TEXT CHECK(id_gender in ('M', 'F', 'X')),  -- reported gender
        pref_hand TEXT CHECK(pref_hand in ('L', 'R'),        -- preferred hand (left or right)
        x_group TEXT CHECK(group in ('A', 'B')),               -- assigned group (A=inertia, B=no inertia)
        e_type TEXT CHECK(e_type in ('sim', 'test', 'pilot', 'real'))    
        -- type of user: sim=simulation, test=debugging/development, 
        -- pilot=pilot study, real=experimental data
    );
```    

## Tools
### datasette

Use [`datasette`](https://datasette.io/) to browse a log file.

`datasette logs.db`

instantly launches a local web server where you can browse tables, run SQL queries and perform basic exploratory analysis with no other setup. This is an invaluable tool for in on the logs as experiments are running, and for doing quick exploratory analyses.

<img src="imgs/datasette.png">

### sqlite-utils

[`sqlite-utils`](https://sqlite-utils.datasette.io/en/stable/) makes it easy to export tables to other formats (primarly CSV/TSV or JSON), and also makes it easy to modify the schema of a live database (e.g. deleting/adding/renaming columns).  While most of these things can be achieved via the sqlite3 command line interface, `sqlite-utils` is generally easier to use and offers some table manipulations that are otherwise very tricky to do.

* `sqlite-utils tables logs.db --counts --table` prints a list of row counts in the logs for each table
* `sqlite-utils analyze-tables logs.db` will print extensive stats on every column of each table, including distinct values, most and least common values, null and blank entries
* `sqlite-utils rows logs.db users --csv > users.csv` will dump the `users` table to `user.csv`
* `sqlite-utils query logs.db "SELECT * FROM LOG WHERE trial_id=7" --json > trial_7.json` will dump the logs for trial 7 to `trial_7.json` (`--csv`, `--tsv` and `--json` specify the output formats)

------

# BESUT-AGATA style pseudonyms

Almost all human subject studies in CS are conducted in a way that personal identity is not stored with experimental data. Users are therefore typically identified by pseudonyms in logs. I use the following code to generate pseudonyms:

```python
import secrets, random, hashlib, sys

def make_pseudo(*args):
    if len(args)==0:
        token = secrets.token_hex(32)  # random
    else:
        h = hashlib.sha512()  # hashed arguments
        h.update(" ".join(sys.argv[1:]).encode("utf-8"))
        token = h.hexdigest()

    random.seed(int(token, 16))
    c, v = "BCDFGHKLMNPRSTVWZ", "UEIOA"
    pattern = [c, v, c, v, c, "-", v, c, v, c, v]
    return "".join(random.choice(s) for s in pattern)

if __name__ == "__main__":
    print(make_pseudo(*sys.argv[1:]))    
```

This generates *pronounceable* random pseudonyms that are long enough that they are unlikely to be confused or mistyped for another, like:

```
MOBAT-ASAHO
MIFUM-OZOPO
KAKOH-EDIHA
TODID-OZUDI
MEDOH-UFORO
CIDIS-AGAVO
```

The script can also take arguments to create a pseudonym from a hash of existing data. When might this be useful? Sometimes there are studies where random anonymous identifiers aren't desirable; instead we can generate a pseudonym of a fixed format from some known information (like a "master" pseudonym and a trial identifier):

```
python pseudo.py user:2160193 trial:4 site:gla

> MEKIR-AHUCA
```

We can always recover the pseudonym from the original arguments, but it is not easy to do so in reverse.

# States and state machines
**Note: this section is less about the process of logging and more about writing experimental software. If you just want to dump numbers to a file, feel free to skip this part!**

Experiments are generally driven by a *state machine*, where state transitions move participants from one part of an experimental trial to another. In the very simplest experiments there are a fixed sequence of tasks that are executed one after the other, identically for all users. More complex experiments will have logical groupings of tasks (e.g. into conditions). Some of these task groupings will only be taken by some subsets of users. Some experiments will have conditional execution of subtasks (such as repeating a trial until a success criterion, or branching tasks depending on previous choices). An subtask might be ended by a user action, or a time out. The ordering of the (some) of the transitions is often shuffled between users or repetitions; for example, counterbalancing condition order to mitigate learning effects in a within-subjects study.

## Experiment states and paths
Experimental software needs to manage the transition between states. These nested state machines are well-represented by Harel's [statecharts](https://statecharts.dev/what-is-a-statechart.html), though drawing the statechart for an experiment is often much too complicated to be worthwhile. It's easier just to implement the code.

### State machine example

These experiment states are typically hierarchically structured and ordered. For example, imagine a Fitts' law style pointing task (testing how fast people can click on targets with a pointing device). We might have

* Two pointing devices: mouse with inertia (A) and mouse without inertia (B)
* Two types of task: static pointing, tracking moving objects
* Five levels of difficulty (in terms of target size/distance for static, target size/speed for tracking)
* Three repetitions of each target acquisition.
* In between each repetition there might be a cooldown and countdown phase

---

* Each user uses *either* the inertia or the without inertia pointing device (between groups)
* Each performs the task type (static, moving) in counterbalanced order. 
* Each pointing task ends when either the user enters the target *or* more than five seconds elapse. 
* We'd have some initial welcome and information screens, and a debriefing page at the very end. 
* Difficulty levels might be randomised with a task. 

This means that the states have an order or subset of valid states which has to be determined for each user. In more complex experiments, the order may only be known at run-time, e.g. if a more difficult task will only be presented if an easier task has already been completed.

### Paths and state transition
* A path-like notation is useful to record which state the software is in: `/A/1/2/1` or even better `/Stage:Main/Pointing:Inertia/Movement:Static/ID:5/Rep:1/Target:3/Phase:Cooldown` 
* The path notation with `attr:value` style elements is particularly helpful when describing orderings or subsets
* Glob-notation is a good way to refer to specific subsets, like `/Stage:*` or `/Stage:Main/Pointing:*/Movement:Static/**`

### Finite state machines
* One useful model has state changes that come in two types:
    * `jump` where an activity jumps to a specific path
    * `next` when an activity finishes normally (or is aborted) and moves to a relative part of the hierarchy based on the ordering of states

* A sequence of the natural state change order needs to be created, usually for each trial, unless the ordering is completely fixed. 
    
* One way of representing these changes is to have functions to change state:    
    * `fsm.jump(reason, path)` jump immediately to the given absolute path, logging `reason` (a string) as the reason for the state change.        
    * `fsm.next(reason, levels=1, normal=True)` when an activity finishes
        * `next("user clicked on target")`
        * `levels` allows `next` to optionally advance "up the hierarchy" e.g. to the next interface type or back to the start of a task.
            * `levels=-1` repeats the last "element"; `levels=1` advances normally,  `levels=0` does nothing, `levels=2` would skip to the start of the next node one level "up"
        * `normal` can be used to flag transitions which are "unnatural", like an trial aborted due to equipment failure
* `src/experiment_states.py` shows how this can be implemented in Python

### States, resuming, context and cache
Some questions that arise in experimental code:
1. How is flow control administered? Does an FSM drive a main body of code, or does the code get driven by the FSM?
1. How do context variables get stored and passed around? How will the logger know that the current user is `BESUT-AGATA`?
1. How will experimental states be saved and resumed? How would we make use of the `saves` table described above?

#### Flow control
My suggestion is to write experimental software that is driven by the state machine, with an explicit "dispatch" loop, like this:

```python
def dispatch(fsm, context, cache):    
    while fsm.state != "COMPLETED" and fsm.state != "ABORTED":
        if fsm.state == "/intro":
            intro(context, cache)
        if fsm.state.startswith("/main"):
            main_experiment(context, cache)
        # etc.
        if fsm.state=="/debrief"
            debrief(context, cache)
        fsm.next(1) # advance to next state
```

This is easier to reason about than spreading state changes about the code (though code could still `jump` or `next` internally if required).

The three parameters I've used are:
* `fsm` the state machine, as above
* `context` a dictionary of variables that represent the current context (like current user id, current trial id) that includes *everything that might need to be serialised to store the current state*.
* `cache` a dictionary of variables (or any other data type) that represents values to carry around that can be reconstructed and shouldn't be persisted (like a network connection reference, database connection, or a file handle).

### Context variables
The `context` dictionary is used to represent the key state of the experiment. A `context` might look like:
```python
{
    "user_id":"BESUT-AGATA",
    "trial_id":7,
    "random_state":[...],
}
```

The experiment can then be `saved`, `loaded` or `inited`:

```python
def save_state(context, cache):
    serialised_context = json.dumps(context)    
    cache.db.execute("INSERT INTO saves VALUES (time, resume_state, user_id, trial_id) VALUES (?,?,?,?)",
            get_time_string(), serialised_context, context["user_id"], context["trial_id"])

def resume_state(cache):
    result = cache.db.execute("SELECT resume_state FROM saves ORDER BY rowid DESC LIMIT 1;").fetchone()
    return json.loads(result)

def init_state(fsm_spec, resume=False, **kwargs):    
    cache = init_cache(**kwargs)
    if not resume:
        # cold start, we need to create both cache and context
        context = init_context(**kwargs)
    else:
        context = resume_state(cache)
    # attach the FSM, with a callback that "knows" the context and cache
    # so has a database handle, etc.
    fsm = FSM(fsm_spec, pre_state_change = 
            lambda prev, new, reason:log_state(prev, new, reason, context, cache))
    if context.current_state != fsm.state:
        fsm.jump(context.current_state, "Resumed state")
    # enable debug logging to database
    attach_debug_log(cache.db)
    return context, cache


def log_state(prev, new, reason, context, cache):
    context.current_state = new
    cache.db.execute("INSERT INTO states (time, prev, new, reason, trial_id) VALUES (?,?,?,?,?)",
            get_time_string(), prev, new, reason, context["trial_id"])

def init_context(user_id, trial_id, **kwargs):
    context = {"user_id":user_id, 
               "trial_id": ...}
    return context

def init_cache(db_file, **kwargs):
    db_connection = sqlite3.connect(db_file)
    cache = {"db":db_connection, ...}
```


### Example
Here's an skeleton example of how the Fitts' law experiment described might be configured and executed using `experiment_states.py`

```python
from experiment_states import FSM

x = State("intro") | 
    State("main") * (            
            State("inertia", ["on", "off"]) * 
            State("target", ["static", "dynamic"]) * 
            State("ID", 5) * State("repeat", 3) * 
            State("phase", ["countdown", "active", "cooldown"])
    )
    | State("debrief")    

def log_state_change(prev, new, reason):
    logger.state_change(prev, new, reason)

fsm = FSM(experiment_tree, pre_state=log_state_change)
fsm.select("inertia", "off")
fsm.order("target", "random")

print(fsm.state) # will be START

def show_intro(fsm):
    # pass    

fsm.jump("/intro", fn=show_intro) 
fsm.next(1)


def main_experiment(self):
    self.next(0) # jump to next valid state


```


# Example

The repository [sql_log_example](http://github.com/johnhw/sql_log_example) has example code and simple scripts for logging. 

```
    sql_log_example/
        add_user.py         # adds a user to the logfile; 
                            # can either use command line arguments or will prompt for info
        backup_log.sh       # backs up the log via rsync
        commit_log.sh       # backs up the log via git
        create_tables.sql   # the schema for a log file
        pseudo.py           # creates pseudonyms
        db_utils.py         # basic utilities for working with a sqlite database in python  
                            # including getting machine info (UUIDs, etc.) timestamps,
                            # dumping tables in a nice format, etc.
        experiment.py       # runs a demo experiment
        simulate.py         # generate a bunch of simulated data
        fresh_log.sh        # moves the database to a backup and, runs `create_tables.sql`
```





[John H Williamson](https://johnhw.github.io)

[GitHub](https://github.com/johnhw) / [@jhnhw](https://twitter.com/jhnhw)

<link rel="stylesheet" href="../global/style.css" type="text/css" ></link>
<script>
window.markdeepOptions = {tocStyle:'short'};</script>
<!-- Markdeep: --><style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style><script src="markdeep.min.js" charset="utf-8"></script><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js" charset="utf-8"></script><script>window.alreadyProcessedMarkdeep||(document.body.style.visibility="visible")</script>